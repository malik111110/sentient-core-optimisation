# Action Plan: Story 1.1.3 - Database Infrastructure & Knowledge Base Setup

**Story ID:** 1.1.3
**Title:** Database Infrastructure & Knowledge Base Setup
**Assigned Agent:** AGENT-DATABASE
**Dependencies:** None (PostgreSQL/Redis can be independent; KB population might depend on T1.1.1 content agents)
**Parent Epic:** 1.1 Multi-Agent Infrastructure Setup

## 1. Acceptance Criteria

- PostgreSQL 16 is set up (Dockerized) and configured with advanced indexing (GIN, GiST) and JSONB support.
- A vector store solution (e.g., Supabase `pgvector` or a dedicated ChromaDB instance) is configured and operational with HNSW indexing if applicable.
- A Redis 7 cluster is deployed and configured for caching and session management.
- A database migration strategy (e.g., using Alembic for PostgreSQL) is defined and initial migrations are in place.
- Automated database backup and recovery procedures are established and tested for PostgreSQL and the vector store.
- A clear framework/API is available for agents to ingest data into and retrieve information (semantic search) from the Knowledge Base.

## 2. Detailed Tasks

This action plan addresses tasks T1.1.3.1 - T1.1.3.8 as defined in `task-breakdown.md` for Story 1.1.3, referencing `supabase-integration.md` (SI) and `multi-agent-architecture.md` (MAA) where relevant for KB concepts.

### Task 1.1.3.1: Set up PostgreSQL 16 Instance (Dockerized)
   - **1.1.3.1.1:** Create a Docker Compose configuration for PostgreSQL 16.
   - **1.1.3.1.2:** Configure persistent storage for PostgreSQL data.
   - **1.1.3.1.3:** Define initial database users, roles, and necessary environment variables (e.g., `POSTGRES_USER`, `POSTGRES_PASSWORD`, `POSTGRES_DB`). (SI Section 5.2.1)
   - **1.1.3.1.4:** Test connectivity to the PostgreSQL instance.

### Task 1.1.3.2: Configure PostgreSQL for Optimal Performance (Indexing, JSONB)
   - **1.1.3.2.1:** Research and apply best-practice configurations for PostgreSQL (e.g., `postgresql.conf` settings for memory, connections).
   - **1.1.3.2.2:** Plan for the use of JSONB for flexible data storage where appropriate (e.g., agent state, logs, metadata).
   - **1.1.3.2.3:** Identify potential use cases for GIN, GiST, or BRIN indexes based on anticipated query patterns.
   - **1.1.3.2.4:** Document PostgreSQL configuration choices and rationale.

### Task 1.1.3.3: Implement and Configure Vector Store (e.g., pgvector or ChromaDB)
   - **1.1.3.3.1:** **Decision Point:** Choose between Supabase `pgvector` (if Supabase is adopted broadly) or a standalone vector DB like ChromaDB.
      - *If `pgvector`*: Enable the `vector` extension in the PostgreSQL instance. (SI Section 2.1, 5.2.2)
      - *If ChromaDB*: Set up ChromaDB instance (e.g., Dockerized) with persistent storage.
   - **1.1.3.3.2:** Configure the chosen vector store for HNSW indexing (if applicable and not default) for efficient similarity search.
   - **1.1.3.3.3:** Define the schema for storing knowledge base entries: `id`, `content_text`, `embedding_vector`, `metadata_jsonb` (source, timestamps, tags, etc.).
   - **1.1.3.3.4:** Test basic vector insertion and similarity search functionality.

### Task 1.1.3.4: Set up Redis 7 Cluster
   - **1.1.3.4.1:** Create a Docker Compose configuration for a Redis 7 cluster (or standalone if sufficient for initial needs).
   - **1.1.3.4.2:** Configure persistence for Redis if required for specific caching strategies or session data.
   - **1.1.3.4.3:** Define connection parameters and ensure accessibility from agent services.
   - **1.1.3.4.4:** Test basic Redis operations (SET, GET, cache expiry).

### Task 1.1.3.5: Define Database Schemas and Implement Migration System (Alembic)
   - **1.1.3.5.1:** For PostgreSQL, define initial database schemas for core application data (e.g., users, agents, tasks, logs if not using a dedicated logging system). Use SQLAlchemy models if an ORM is planned for agent services.
   - **1.1.3.5.2:** Set up Alembic for managing PostgreSQL schema migrations.
   - **1.1.3.5.3:** Create initial Alembic migration scripts for the defined schemas.
   - **1.1.3.5.4:** Document the schema design and migration process.

### Task 1.1.3.6: Establish Automated Database Backup and Recovery Procedures
   - **1.1.3.6.1:** Implement automated backup scripts/tools for PostgreSQL (e.g., `pg_dump`, `pgBackRest`).
   - **1.1.3.6.2:** Implement backup procedures for the chosen vector store (if not covered by PostgreSQL backups, e.g., for standalone ChromaDB).
   - **1.1.3.6.3:** Define backup frequency, retention policies, and storage locations (e.g., S3 bucket).
   - **1.1.3.6.4:** Document and test database recovery procedures from backups.

### Task 1.1.3.7: Design and Implement API/Service for KB Ingestion and Retrieval
   - **1.1.3.7.1:** Design a service (e.g., FastAPI microservice) or a set of library functions for interacting with the Knowledge Base.
   - **1.1.3.7.2:** **Ingestion API:** Endpoints/functions for adding documents/text to the KB. This should handle:
      - Text cleaning and preprocessing.
      - Document chunking (strategies: fixed size, sentence-based, recursive).
      - Embedding generation using a chosen model (e.g., OpenAI `text-embedding-ada-002`, Sentence Transformers).
      - Storing text chunks, embeddings, and metadata in the vector store.
   - **1.1.3.7.3:** **Retrieval API:** Endpoints/functions for agents to query the KB:
      - Takes a natural language query.
      - Generates an embedding for the query.
      - Performs similarity search (e.g., cosine similarity) against the vector store.
      - Returns relevant chunks/documents with metadata.
      - Optionally implement re-ranking or filtering logic.
   - **1.1.3.7.4:** Secure the API endpoints (e.g., with API keys or JWTs if it's a separate service).

### Task 1.1.3.8: Create Initial KB Population Mechanism
   - **1.1.3.8.1:** Identify initial sources for the Knowledge Base (e.g., project documentation like `applied techs/*.md`, core framework documentation, relevant technical articles).
   - **1.1.3.8.2:** Develop scripts or a simple process to use the Ingestion API (from T1.1.3.7.2) to populate the KB with this initial content.
   - **1.1.3.8.3:** Verify that the content is correctly ingested, chunked, embedded, and retrievable.
   - **1.1.3.8.4:** Document the KB population process and sources.

## 3. Key Considerations & References

- **Data Security:** Ensure all database instances and APIs are secured. Use strong credentials, network policies, and encryption where appropriate. For KB access, consider RLS if using Supabase/PostgreSQL directly. (SI Section 5.3).
- **Scalability:** Design database schemas and choose configurations with future scalability in mind.
- **Embedding Model Choice:** The choice of embedding model will impact retrieval quality and cost. Evaluate options based on performance and domain specificity.
- **Chunking Strategy:** Effective document chunking is crucial for good retrieval performance. Experiment with different strategies.
- **Metadata Usage:** Rich metadata associated with KB entries can significantly improve filtering and contextual retrieval.
- **References:**
    - `supabase-integration.md` (SI): PostgreSQL setup (5.2), `pgvector` (2.1, 5.2.2), RLS for security (5.3).
    - `multi-agent-architecture.md` (MAA): General concepts for agent interaction with data stores, though less specific to DB setup.
    - Official documentation for PostgreSQL, Redis, Alembic, and the chosen vector store (ChromaDB, pgvector).
